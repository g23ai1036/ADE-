## This will include some additional set up details as needed for ETL processing in addition to Assignment 1


1. Loading the data sets from local machine to s3 bucket
  
  1.1 Download the data set from Kaggle
      Save it in a folder  

2. Load the data 
  
  2.1 Create a folder and subfolder in s3 and Copy all json files from local to s3 in the subfloder
      
       Open the cmd terminal and navigate to the directory where data is saved
       Type the following command: aws s3 cp . s3://bucketname/foldername/raw_statistics_reference_data/ --recursive --exclude "*" --include "*.json" 
                                   aws s3 cp . s3://ade-project-shirish/youtube/raw_statistics_reference_data/ --recursive --exclude "*" --include "*.json"

      This does the following:
        Create a folder named youtube
        Create a Subfolder named raw_Statistics_reference_data
        Copies all the files with .jason extension from local directory to the subfolder
        
        


2.2 # To copy all data files run the following commands:

  
          aws s3 cp CAvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=ca/
          aws s3 cp DEvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=de/
          aws s3 cp FRvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=fr/
          aws s3 cp GBvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=gb/
          aws s3 cp INvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=in/
          aws s3 cp JPvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=jp/
          aws s3 cp KRvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=kr/
          aws s3 cp MXvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=mx/
          aws s3 cp RUvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=ru/
          aws s3 cp USvideos.csv s3://ade-project-shirish/youtube/raw_statistics/region=us/

    These commands do the following:
      Create a subfolder raw_statistics in youtube folder
      create a separate subfolder for each region and copy the respective file to the respective subfolder
      upload all the .csv files 

3. AWS Glue

3.1 Add Crawler  
    Launch Glue
    Create Crawler
    Give a name
    Add a data source
      Browse s3 path
      Select reference data folder with the project, which has a group of jason files
    Select crawl all subfloders 
      - Crawler will pull the meta data from all the subfolders in S3 and table them in data catalog
  3.1.1 Add IAM role
        Launch IAM
        Create a new role
        Assign S3 Full services - crawler will run through s3 folders and pull the meta data from all to table the in a data catalog
        Queries can be run on the data catalog then
        Open the role link and attach policy to add another policy to access GlueService role   
        Once created, come back to crawler and select the role in crawler creation

  3.1.2 Add a data base
        Database needs to be added for glue to store the metadata after scanning through the s3 buckets and pulling the meta data
        once created, refresh and select the database
        go to the next tab and create the crawler

  3.1.3  Run the crawler
        Running the crawler created the following:
          1.  A database in data catalog
          2.  A table 

  3.1.4 View the table in data catalog launches Athena 
        Running the query in query editor gave the following error message:
        " HIVE_CURSOR_ERROR: Row is not a valid JSON Object - JSONException: A JSONObject text must end with '}' at 2 [character 3 line 1]"


 
    
      
